{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of 1_codingtest2_baseline_code.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sfsf9797/ML_FROM_SCRATCH/blob/master/ImageSegmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3g-RkP76VEi9"
      },
      "source": [
        "# This baseline code requires a GPU device. If you have not selected the GPU runtime type, you can change the runtime type to enable GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYtzlI24E5-Y",
        "outputId": "36de2935-7d63-4ad5-cdb9-cebfe1d420e3"
      },
      "source": [
        "!pip install  --upgrade albumentations"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: albumentations in /usr/local/lib/python3.7/dist-packages (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from albumentations) (4.5.2.52)\n",
            "Requirement already satisfied, skipping upgrade: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: scikit-image>=0.16.1 in /usr/local/lib/python3.7/dist-packages (from albumentations) (0.16.2)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (2.5.1)\n",
            "Requirement already satisfied, skipping upgrade: pillow>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (1.1.1)\n",
            "Requirement already satisfied, skipping upgrade: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (2.4.1)\n",
            "Requirement already satisfied, skipping upgrade: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit-image>=0.16.1->albumentations) (4.4.2)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xif7iKbpsbrH",
        "outputId": "c904d812-f622-4d75-f33d-1db61754e437"
      },
      "source": [
        "import os\n",
        "import random\n",
        "import time\n",
        "import json\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import cv2\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Python package for pre-processing \n",
        "from pycocotools.coco import COCO\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "# Python package for visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set()\n",
        "\n",
        "\n",
        "\n",
        "plt.rcParams['axes.grid'] = False\n",
        "\n",
        "print('Pytorch version: {}'.format(torch.__version__))\n",
        "print('Is GPU available: {}'.format(torch.cuda.is_available()))\n",
        "if torch.cuda.is_available():\n",
        "  print(torch.cuda.get_device_name(0))\n",
        "  print('The number of GPUs available: {}'.format(torch.cuda.device_count()))\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" \n",
        "\n",
        "print('CPU count: {}'.format(os.cpu_count()))  # 2"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pytorch version: 1.8.1+cu101\n",
            "Is GPU available: True\n",
            "Tesla T4\n",
            "The number of GPUs available: 1\n",
            "CPU count: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBA68zt_3CmU"
      },
      "source": [
        "# Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R63UKbH0wOIi",
        "outputId": "fdc5e559-29c1-448b-fc78-2a90336de99e"
      },
      "source": [
        "!wget https://aistages-public-junyeop.s3.amazonaws.com/app/Competitions/000015/data/data.tar.gz\n",
        "!mkdir input\n",
        "!tar -xzf data.tar.gz -C ./input\n",
        "!rm data.tar.gz"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-07 10:27:05--  https://aistages-public-junyeop.s3.amazonaws.com/app/Competitions/000015/data/data.tar.gz\n",
            "Resolving aistages-public-junyeop.s3.amazonaws.com (aistages-public-junyeop.s3.amazonaws.com)... 52.219.58.6\n",
            "Connecting to aistages-public-junyeop.s3.amazonaws.com (aistages-public-junyeop.s3.amazonaws.com)|52.219.58.6|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 490693783 (468M) [application/x-gzip]\n",
            "Saving to: ‘data.tar.gz’\n",
            "\n",
            "data.tar.gz         100%[===================>] 467.96M  16.6MB/s    in 30s     \n",
            "\n",
            "2021-06-07 10:27:36 (15.5 MB/s) - ‘data.tar.gz’ saved [490693783/490693783]\n",
            "\n",
            "mkdir: cannot create directory ‘input’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7Da2Qduu-j2"
      },
      "source": [
        "# Set the hyperparameters and fix the seed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GW6wr99nubZ8"
      },
      "source": [
        "batch_size = 8  # Mini-batch size\n",
        "num_epochs = 25\n",
        "learning_rate = 5e-04"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldCLgwXLvOAo"
      },
      "source": [
        "# fix the seed\n",
        "random_seed = 21\n",
        "torch.manual_seed(random_seed)\n",
        "torch.cuda.manual_seed(random_seed)\n",
        "torch.cuda.manual_seed_all(random_seed) # if use multi-GPU\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(random_seed)\n",
        "random.seed(random_seed)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hjuiair7vV4J"
      },
      "source": [
        "# EDA of the training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ffls0hfvSC-",
        "outputId": "27cff53d-0943-4cd3-b28e-68e772d79145"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "dataset_path = 'input'\n",
        "anns_file_path = os.path.join(dataset_path, 'train.json')\n",
        "\n",
        "# Read annotations\n",
        "with open(anns_file_path, 'r') as f:\n",
        "    dataset = json.loads(f.read())\n",
        "\n",
        "categories = dataset['categories']\n",
        "anns = dataset['annotations']\n",
        "imgs = dataset['images']\n",
        "nr_cats = len(categories)\n",
        "nr_annotations = len(anns)\n",
        "nr_images = len(imgs)\n",
        "\n",
        "# Load categories and super categories\n",
        "cat_names = []\n",
        "super_cat_names = []\n",
        "super_cat_ids = {}\n",
        "super_cat_last_name = ''\n",
        "nr_super_cats = 0\n",
        "for cat_it in categories:\n",
        "    cat_names.append(cat_it['name'])\n",
        "    super_cat_name = cat_it['supercategory']\n",
        "    # Adding new supercat\n",
        "    if super_cat_name != super_cat_last_name:\n",
        "        super_cat_names.append(super_cat_name)\n",
        "        super_cat_ids[super_cat_name] = nr_super_cats\n",
        "        super_cat_last_name = super_cat_name\n",
        "        nr_super_cats += 1\n",
        "\n",
        "print('Number of super categories:', nr_super_cats)\n",
        "print('Number of categories:', nr_cats)\n",
        "print('Number of annotations:', nr_annotations)\n",
        "print('Number of images:', nr_images)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of super categories: 11\n",
            "Number of categories: 11\n",
            "Number of annotations: 21116\n",
            "Number of images: 2617\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "EOtpD5js3Vy5",
        "outputId": "a283aabc-65af-4c66-88b3-9a9351422e5c"
      },
      "source": [
        "# Count annotations\n",
        "cat_histogram = np.zeros(nr_cats,dtype=int)\n",
        "for ann in anns:\n",
        "    cat_histogram[ann['category_id']] += 1\n",
        "\n",
        "# Initialize the matplotlib figure\n",
        "f, ax = plt.subplots(figsize=(5,5))\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame({'Categories': cat_names, 'Number of annotations': cat_histogram})\n",
        "df = df.sort_values('Number of annotations', 0, False)\n",
        "\n",
        "# Plot the histogram\n",
        "plt.title(\"category distribution of train set \")\n",
        "plot_1 = sns.barplot(x=\"Number of annotations\", y=\"Categories\", data=df, label=\"Total\", color=\"b\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "XecVByfN3Wuy",
        "outputId": "50d6d53f-714f-4a8b-f575-0231b15e46f8"
      },
      "source": [
        "sorted_temp_df = df.sort_index()\n",
        "sorted_df = pd.DataFrame([\"Backgroud\"], columns = [\"Categories\"])\n",
        "sorted_df = sorted_df.append(sorted_temp_df, ignore_index=True)\n",
        "sorted_df"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Categories</th>\n",
              "      <th>Number of annotations</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Backgroud</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>UNKNOWN</td>\n",
              "      <td>128.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>General trash</td>\n",
              "      <td>2225.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Paper</td>\n",
              "      <td>7448.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Paper pack</td>\n",
              "      <td>527.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Metal</td>\n",
              "      <td>449.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Glass</td>\n",
              "      <td>488.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Plastic</td>\n",
              "      <td>2472.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Styrofoam</td>\n",
              "      <td>1074.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Plastic bag</td>\n",
              "      <td>6114.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Battery</td>\n",
              "      <td>50.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Clothing</td>\n",
              "      <td>141.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Categories  Number of annotations\n",
              "0       Backgroud                    NaN\n",
              "1         UNKNOWN                  128.0\n",
              "2   General trash                 2225.0\n",
              "3           Paper                 7448.0\n",
              "4      Paper pack                  527.0\n",
              "5           Metal                  449.0\n",
              "6           Glass                  488.0\n",
              "7         Plastic                 2472.0\n",
              "8       Styrofoam                 1074.0\n",
              "9     Plastic bag                 6114.0\n",
              "10        Battery                   50.0\n",
              "11       Clothing                  141.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxWCzu1-_dVD"
      },
      "source": [
        "# Make a CustomDataset class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVIVLWiT_ceM"
      },
      "source": [
        "category_names = list(sorted_df.Categories)\n",
        "\n",
        "def get_classname(classID, cats):\n",
        "    for i in range(len(cats)):\n",
        "        if cats[i]['id']==classID:\n",
        "            return cats[i]['name']\n",
        "    return \"None\"\n",
        "\n",
        "def mask2arr(mask):\n",
        "    arr = np.zeros((12,mask.shape[0],mask.shape[1]))\n",
        "    for i in range(12):\n",
        "        arr[i,:,:] = np.where(mask==i,1,0)\n",
        "    return torch.from_numpy(arr)\n",
        "\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    \"\"\"COCO format\"\"\"\n",
        "    def __init__(self, data_dir, mode = 'train', transform = None,preprocessing=None):\n",
        "        super().__init__()\n",
        "        self.mode = mode\n",
        "        self.transform = transform\n",
        "        self.coco = COCO(data_dir)\n",
        "        self.preprocessing = preprocessing\n",
        "        \n",
        "        \n",
        "    def __getitem__(self, index: int):\n",
        "        # Get the image_info using coco library\n",
        "        image_id = self.coco.getImgIds(imgIds=index)\n",
        "        image_infos = self.coco.loadImgs(image_id)[0]\n",
        "\n",
        "        # Load the image using opencv\n",
        "        images = cv2.imread(os.path.join(dataset_path, image_infos['file_name']))\n",
        "        images = cv2.cvtColor(images, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
        "        images /= 255.0\n",
        "        \n",
        "        if (self.mode in ('train', 'val')):\n",
        "            ann_ids = self.coco.getAnnIds(imgIds=image_infos['id'])\n",
        "            anns = self.coco.loadAnns(ann_ids)\n",
        "            # print(\"image_infos['id'] : {}\".format(image_infos['id']) )\n",
        "            # Load the categories in a variable\n",
        "            cat_ids = self.coco.getCatIds()\n",
        "            cats = self.coco.loadCats(cat_ids)\n",
        "\n",
        "            # masks_size : height x width            \n",
        "            masks = np.zeros((image_infos[\"height\"], image_infos[\"width\"]))\n",
        "            for i in range(len(anns)):\n",
        "                className = get_classname(anns[i]['category_id'], cats)\n",
        "                pixel_value = category_names.index(className)\n",
        "                masks = np.maximum(self.coco.annToMask(anns[i])*pixel_value, masks)\n",
        "            masks = masks.astype(np.float32)\n",
        "            # Background = 0, Unknown = 1, General trash = 2, ... , Cigarette = 11\n",
        "            # for i in range(len(anns)):\n",
        "            #     id=anns[i]['category_id']+1\n",
        "            #     masks[id,:,:] = np.maximum(self.coco.annToMask(anns[i]), masks[id,:,:])\n",
        "\n",
        "            # masks[0,:,:] = np.sum(masks[1:,:,:],axis=0)\n",
        "            # masks[0,:,:] = np.where(masks[0,:,:]>=1,0,1)\n",
        "            # masks = masks.astype(np.float32)\n",
        "\n",
        "            # We can use Albumentations for image & mask transformation(or augmentation)\n",
        "            if self.transform is not None:\n",
        "                transformed = self.transform(image=images, mask=masks)\n",
        "                images = transformed[\"image\"]\n",
        "                masks = transformed[\"mask\"]\n",
        "                masks =  masks.squeeze()\n",
        "            if self.preprocessing:\n",
        "                preprocessed = self.preprocessing(image=images, mask=masks)\n",
        "                images = preprocessed['image']\n",
        "                masks = preprocessed['mask']\n",
        "       \n",
        "           # print(masks.shape)\n",
        "            masks = mask2arr(masks)\n",
        "            \n",
        "            return images.type(torch.FloatTensor), masks.type(torch.FloatTensor) #, image_infos\n",
        "        \n",
        "        if self.mode == 'test':            \n",
        "            if self.transform is not None:\n",
        "                transformed = self.transform(image=images)\n",
        "                images = transformed[\"image\"]\n",
        "            \n",
        "            return images.type(torch.FloatTensor), image_infos\n",
        "    \n",
        "    \n",
        "    def __len__(self) -> int:        \n",
        "        return len(self.coco.getImgIds())"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SCcpeQphi_J"
      },
      "source": [
        "[{'id': 0, 'name': 'UNKNOWN', 'supercategory': 'UNKNOWN'},\n",
        " {'id': 1, 'name': 'General trash', 'supercategory': 'General trash'},\n",
        " {'id': 2, 'name': 'Paper', 'supercategory': 'Paper'},\n",
        " {'id': 3, 'name': 'Paper pack', 'supercategory': 'Paper pack'},\n",
        " {'id': 4, 'name': 'Metal', 'supercategory': 'Metal'},\n",
        " {'id': 5, 'name': 'Glass', 'supercategory': 'Glass'},\n",
        " {'id': 6, 'name': 'Plastic', 'supercategory': 'Plastic'},\n",
        " {'id': 7, 'name': 'Styrofoam', 'supercategory': 'Styrofoam'},\n",
        " {'id': 8, 'name': 'Plastic bag', 'supercategory': 'Plastic bag'},\n",
        " {'id': 9, 'name': 'Battery', 'supercategory': 'Battery'},\n",
        " {'id': 10, 'name': 'Clothing', 'supercategory': 'Clothing'}]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGx9Byge_9Ab"
      },
      "source": [
        "# Create instances of CustomDataset and Assign it to DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIQtybXTAa3Q"
      },
      "source": [
        "train_path = os.path.join(dataset_path, 'train.json')\n",
        "val_path = os.path.join(dataset_path, 'val.json')\n",
        "test_path = os.path.join(dataset_path, 'test.json')\n",
        "\n",
        "# collate_fn needs for batch\n",
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "def get_training_augmentation():\n",
        "    train_transform = [\n",
        "       # A.CropNonEmptyMaskIfExists(320, 480),\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.VerticalFlip(p=0.5),\n",
        "        A.ShiftScaleRotate(\n",
        "            scale_limit=0.5,\n",
        "            rotate_limit=0,\n",
        "            shift_limit=0.1,\n",
        "            p=0.5,\n",
        "            border_mode=0\n",
        "        ),\n",
        "        A.GridDistortion(p=0.5),\n",
        "       # A.Resize(400, 400),\n",
        "       # A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "        ToTensorV2(),\n",
        "    ]\n",
        "    return A.Compose(train_transform)\n",
        "\n",
        "\n",
        "def get_validation_augmentation():\n",
        "    \"\"\"Add paddings to make image shape divisible by 32\"\"\"\n",
        "    test_transform = [\n",
        "        A.Resize(512, 512),\n",
        "     #  A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "       ToTensorV2(),\n",
        "    ]\n",
        "    return A.Compose(test_transform)\n",
        "\n",
        "test_transform = A.Compose([    \n",
        "    ToTensorV2(),\n",
        "])\n",
        "\n",
        "def to_tensor(x, **kwargs):\n",
        "    \"\"\"\n",
        "    Convert image or mask.\n",
        "    \"\"\"\n",
        "    return x.transpose(2, 0, 1).astype('float32')\n",
        "\n",
        "\n",
        "def get_preprocessing(preprocessing_fn):\n",
        "    \"\"\"Construct preprocessing transform\n",
        "    \n",
        "    Args:\n",
        "        preprocessing_fn (callbale): data normalization function \n",
        "            (can be specific for each pretrained neural network)\n",
        "    Return:\n",
        "        transform: albumentations.Compose\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    _transform = [\n",
        "        A.Lambda(image=preprocessing_fn),\n",
        "         ToTensorV2()\n",
        "       # A.Lambda(image=to_tensor, mask=to_tensor),\n",
        "    ]\n",
        "    return A.Compose(_transform)\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VViUutteiZhB"
      },
      "source": [
        "# def mask2arr(mask):\n",
        "#    arr = np.zeros((12,mask.shape[0],mask.shape[1]))\n",
        "#    for i in range(12):\n",
        "#      arr[i,:,:] = np.where(mask==i,1,0)\n",
        "#    return arr\n",
        "\n",
        "def arr2mask(arr):\n",
        "    batch = arr.shape[0]\n",
        "    mask = np.zeros((batch, arr.shape[2],arr.shape[3]))\n",
        "    for b in range(batch):\n",
        "        for i in range(12):\n",
        "            mask[b,:,:] = np.where(arr[b,i,:,:]==1,i,mask[b,:,:])\n",
        "    return mask\n",
        "\n",
        "def arr2mask2D(arr):\n",
        "  \n",
        "    mask = np.zeros((arr.shape[1],arr.shape[2]))\n",
        "    for i in range(12):\n",
        "        mask[:,:] = np.where(arr[i,:,:]==1,i,mask[:,:])\n",
        "    return mask"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fm0z0lgpnPus"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6FU4lGEBEdX"
      },
      "source": [
        "# Show a sample by dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znx8n1cWzi67"
      },
      "source": [
        "### train_loader\n",
        "This train_loader loads the data for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzUKb9kFBGaC"
      },
      "source": [
        "# for imgs, masks, image_infos in train_loader:\n",
        "#     image_infos = image_infos[0]\n",
        "#     temp_images = imgs\n",
        "#     temp_masks = masks\n",
        "#     break\n",
        "\n",
        "# fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(12, 12))\n",
        "\n",
        "# print('image shape:', list(temp_images[0].shape))\n",
        "# print('mask shape: ', list(temp_masks[0].shape))\n",
        "# print('Unique values, category of transformed mask : \\n', [{int(i),category_names[int(i)]} for i in list(np.unique(arr2mask2D(temp_masks[0])))])\n",
        "\n",
        "# ax1.imshow(temp_images[0].permute([1,2,0]))\n",
        "# ax1.grid(False)\n",
        "# ax1.set_title(\"input image : {}\".format(image_infos['file_name']), fontsize = 15)\n",
        "\n",
        "# ax2.imshow(arr2mask2D(temp_masks[0]))\n",
        "# ax2.grid(False)\n",
        "# ax2.set_title(\"masks : {}\".format(image_infos['file_name']), fontsize = 15)\n",
        "\n",
        "# plt.show()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nak6s1xZEtsH"
      },
      "source": [
        "# Baseline model\n",
        "- Here, we show you the UNet with EfficientNet-B0 encoder model. \n",
        "- We just built this baseline model with using a small model, no image augmentation, no fine-tuned hyper-parameters, etc.\n",
        "- you can build your model based on this baseline model and you can improve the model in your own way."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tt1FaTbk5Pjd",
        "outputId": "4ebf679c-cff4-4b05-a337-d542ce9528de"
      },
      "source": [
        "!pip install git+https://github.com/qubvel/segmentation_models.pytorch\n",
        "\n",
        "import segmentation_models_pytorch as smp"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/qubvel/segmentation_models.pytorch\n",
            "  Cloning https://github.com/qubvel/segmentation_models.pytorch to /tmp/pip-req-build-xs81u1o3\n",
            "  Running command git clone -q https://github.com/qubvel/segmentation_models.pytorch /tmp/pip-req-build-xs81u1o3\n",
            "Requirement already satisfied (use --upgrade to upgrade): segmentation-models-pytorch==0.1.3 from git+https://github.com/qubvel/segmentation_models.pytorch in /usr/local/lib/python3.7/dist-packages\n",
            "Requirement already satisfied: torchvision>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from segmentation-models-pytorch==0.1.3) (0.9.1+cu101)\n",
            "Requirement already satisfied: pretrainedmodels==0.7.4 in /usr/local/lib/python3.7/dist-packages (from segmentation-models-pytorch==0.1.3) (0.7.4)\n",
            "Requirement already satisfied: efficientnet-pytorch==0.6.3 in /usr/local/lib/python3.7/dist-packages (from segmentation-models-pytorch==0.1.3) (0.6.3)\n",
            "Requirement already satisfied: timm==0.3.2 in /usr/local/lib/python3.7/dist-packages (from segmentation-models-pytorch==0.1.3) (0.3.2)\n",
            "Requirement already satisfied: torch==1.8.1 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.3.0->segmentation-models-pytorch==0.1.3) (1.8.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.3.0->segmentation-models-pytorch==0.1.3) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.3.0->segmentation-models-pytorch==0.1.3) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch==0.1.3) (4.41.1)\n",
            "Requirement already satisfied: munch in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch==0.1.3) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1->torchvision>=0.3.0->segmentation-models-pytorch==0.1.3) (3.7.4.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from munch->pretrainedmodels==0.7.4->segmentation-models-pytorch==0.1.3) (1.15.0)\n",
            "Building wheels for collected packages: segmentation-models-pytorch\n",
            "  Building wheel for segmentation-models-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segmentation-models-pytorch: filename=segmentation_models_pytorch-0.1.3-cp37-none-any.whl size=83178 sha256=ffe869b30dcddc0cc9d6c6a235591ea06b2c3943b3ff7d1653ed516a20abc76f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-2o1oj0rp/wheels/79/3f/09/1587a252e0314d26ad242d6d2e165622ab95c95e5cfe4b942c\n",
            "Successfully built segmentation-models-pytorch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWMfF6dDEtVx"
      },
      "source": [
        "ENCODER = 'efficientnet-b4'\n",
        "ENCODER_WEIGHTS = 'imagenet'\n",
        "DEVICE = 'cuda'\n",
        "\n",
        "ACTIVATION = 'softmax2d'\n",
        "model = smp.Unet(\n",
        "    encoder_name=ENCODER, \n",
        "    encoder_weights=ENCODER_WEIGHTS, \n",
        "    classes=12, \n",
        "    activation=ACTIVATION,\n",
        ")\n",
        "model.to(DEVICE)\n",
        "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WX_Z-xxE47b"
      },
      "source": [
        "# Define the train, validation, test functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzJ-qNJCEw62"
      },
      "source": [
        "def f_score(pr, gt, beta=1, eps=1e-7, threshold=None, activation='sigmoid'):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        pr (torch.Tensor): A list of predicted elements\n",
        "        gt (torch.Tensor):  A list of elements that are to be predicted\n",
        "        eps (float): epsilon to avoid zero division\n",
        "        threshold: threshold for outputs binarization\n",
        "    Returns:\n",
        "        float: IoU (Jaccard) score\n",
        "    \"\"\"\n",
        "\n",
        "    if activation is None or activation == \"none\":\n",
        "        activation_fn = lambda x: x\n",
        "    elif activation == \"sigmoid\":\n",
        "        activation_fn = torch.nn.Sigmoid()\n",
        "    elif activation == \"softmax2d\":\n",
        "        activation_fn = torch.nn.Softmax2d()\n",
        "    else:\n",
        "        raise NotImplementedError(\n",
        "            \"Activation implemented for sigmoid and softmax2d\"\n",
        "        )\n",
        "\n",
        "    pr = activation_fn(pr)\n",
        "\n",
        "    if threshold is not None:\n",
        "        pr = (pr > threshold).float()\n",
        "\n",
        "\n",
        "    tp = torch.sum(gt * pr)\n",
        "    fp = torch.sum(pr) - tp\n",
        "    fn = torch.sum(gt) - tp\n",
        "\n",
        "    score = ((1 + beta ** 2) * tp + eps) \\\n",
        "            / ((1 + beta ** 2) * tp + beta ** 2 * fn + fp + eps)\n",
        "\n",
        "    return score\n",
        "\n",
        "\n",
        "class DiceLoss(nn.Module):\n",
        "    __name__ = 'dice_loss'\n",
        "\n",
        "    def __init__(self, eps=1e-7, activation='sigmoid'):\n",
        "        super().__init__()\n",
        "        self.activation = activation\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, y_pr, y_gt):\n",
        "        return 1 - f_score(y_pr, y_gt, beta=1., \n",
        "                           eps=self.eps, threshold=None, \n",
        "                           activation=self.activation)\n",
        "\n",
        "\n",
        "class BCEDiceLoss(DiceLoss):\n",
        "    __name__ = 'bce_dice_loss'\n",
        "\n",
        "    def __init__(self, eps=1e-7, activation='sigmoid', lambda_dice=1.0, lambda_bce=1.0):\n",
        "        super().__init__(eps, activation)\n",
        "        if activation == None:\n",
        "            self.bce = nn.BCELoss(reduction='mean')\n",
        "        else:\n",
        "            self.bce = nn.BCEWithLogitsLoss(reduction='mean')\n",
        "        self.lambda_dice=lambda_dice\n",
        "        self.lambda_bce=lambda_bce\n",
        "\n",
        "    def forward(self, y_pr, y_gt):\n",
        "        dice = super().forward(y_pr, y_gt)\n",
        "        bce = self.bce(y_pr, y_gt)\n",
        "        return (self.lambda_dice*dice) + (self.lambda_bce* bce)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEgmQniSI2Xc"
      },
      "source": [
        "# define the evaluation function\n",
        "# https://github.com/wkentaro/pytorch-fcn/blob/master/torchfcn/utils.py\n",
        "import numpy as np\n",
        "\n",
        "def _fast_hist(label_true, label_pred, n_class):\n",
        "    mask = (label_true >= 0) & (label_true < n_class)\n",
        "    hist = np.bincount(\n",
        "        n_class * label_true[mask].astype(int) +\n",
        "        label_pred[mask], minlength=n_class ** 2).reshape(n_class, n_class)\n",
        "    return hist\n",
        "\n",
        "\n",
        "def label_accuracy_score(label_trues, label_preds, n_class=12):\n",
        "    \"\"\"Returns accuracy score evaluation result.\n",
        "      - overall accuracy\n",
        "      - mean accuracy\n",
        "      - mean IU\n",
        "      - fwavacc\n",
        "    \"\"\"\n",
        "    # label_trues = arr2mask(label_trues.detach().cpu().numpy())\n",
        "    # label_preds = torch.argmax(label_preds.squeeze(), dim=1).detach().cpu().numpy()\n",
        "    hist = np.zeros((n_class, n_class))\n",
        "    for lt, lp in zip(label_trues, label_preds):\n",
        "        hist += _fast_hist(lt.flatten(), lp.flatten(), n_class)\n",
        "    acc = np.diag(hist).sum() / hist.sum()\n",
        "    with np.errstate(divide='ignore', invalid='ignore'):\n",
        "        acc_cls = np.diag(hist) / hist.sum(axis=1)\n",
        "    acc_cls = np.nanmean(acc_cls)\n",
        "    with np.errstate(divide='ignore', invalid='ignore'):\n",
        "        iu = np.diag(hist) / (\n",
        "            hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist)\n",
        "        )\n",
        "    mean_iu = np.nanmean(iu)\n",
        "\n",
        "    return  mean_iu"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GU5fR2sxE-Rs"
      },
      "source": [
        "# Define the function to save the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktxlRlASE8Vl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2708981d-2e6b-4684-80e4-87ccd5fb121d"
      },
      "source": [
        "train_dataset = CustomDataset(data_dir=train_path, mode='train', transform=get_training_augmentation())  #,preprocessing=get_preprocessing(preprocessing_fn)\n",
        "\n",
        "\n",
        "# validation dataset\n",
        "val_dataset = CustomDataset(data_dir=val_path, mode='val', transform=get_validation_augmentation()) #,preprocessing=get_preprocessing(preprocessing_fn)\n",
        "\n",
        "\n",
        "# test dataset\n",
        "test_dataset = CustomDataset(data_dir=test_path, mode='test', transform=test_transform) #,preprocessing=get_preprocessing(preprocessing_fn)\n",
        "\n",
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True,\n",
        "                                           num_workers=1,\n",
        "                                          )\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
        "                                         batch_size=batch_size,\n",
        "                                         shuffle=True,\n",
        "                                         num_workers=1,\n",
        "                                        )\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=4,\n",
        "                                          num_workers=1,\n",
        "                                          collate_fn=collate_fn)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=3.33s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.67s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.02s)\n",
            "creating index...\n",
            "index created!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3CfXbRUZGgW"
      },
      "source": [
        "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\n",
        "\n",
        "#model = smp.Unet(encoder_name='efficientnet-b4', classes=12 , encoder_weights=\"imagenet\")\n",
        "loss = BCEDiceLoss()\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam([\n",
        "    {'params': model.decoder.parameters(), 'lr': 5e-2}, \n",
        "    {'params': model.encoder.parameters(), 'lr': 5e-3},  \n",
        "])\n",
        "#scheduler = ReduceLROnPlateau(optimizer, factor=0.15, patience=2)\n",
        "\n",
        "metrics = [\n",
        "    #label_accuracy_score,      \n",
        "    smp.utils.metrics.IoU(threshold=0.7),\n",
        "]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KwOBR4hZkIz"
      },
      "source": [
        "train_epoch = smp.utils.train.TrainEpoch(\n",
        "    model, \n",
        "    loss=loss, \n",
        "    metrics=metrics, \n",
        "    optimizer=optimizer,\n",
        "    device=DEVICE,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "valid_epoch = smp.utils.train.ValidEpoch(\n",
        "    model, \n",
        "    loss=loss, \n",
        "    metrics=metrics, \n",
        "    device=DEVICE,\n",
        "    verbose=True,\n",
        ")"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 908
        },
        "id": "fAiYHDZt5pFN",
        "outputId": "e9c678cc-88e8-4ba5-9c86-cdfbaa36df52"
      },
      "source": [
        "max_score = 0\n",
        "\n",
        "for i in range(0, 15):\n",
        "    print('\\nEpoch: {}'.format(i))\n",
        "    train_logs = train_epoch.run(train_loader)\n",
        "    valid_logs = valid_epoch.run(val_loader)\n",
        "    \n",
        "    # do something (save model, change lr, etc.)\n",
        "    if max_score < valid_logs['iou_score']:\n",
        "        max_score = valid_logs['iou_score']\n",
        "        torch.save(model, './best_model.pth')\n",
        "        print('Model saved!')\n",
        "        \n",
        "    if i == 5:\n",
        "        optimizer.param_groups[0]['lr'] = 1e-3\n",
        "        print('Decrease decoder learning rate to 1e-3!')\n",
        "\n",
        "    if i == 10:\n",
        "      optimizer.param_groups[0]['lr'] = 1e-5\n",
        "      print('Decrease decoder learning rate to 1e-5!')\n",
        "      \n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 0\n",
            "train: 100%|██████████| 328/328 [06:19<00:00,  1.16s/it, bce_dice_loss - 1.502, iou_score - 0.5827]\n",
            "valid: 100%|██████████| 82/82 [00:51<00:00,  1.58it/s, bce_dice_loss - 1.501, iou_score - 0.5652]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 1\n",
            "train: 100%|██████████| 328/328 [06:19<00:00,  1.16s/it, bce_dice_loss - 1.494, iou_score - 0.6288]\n",
            "valid: 100%|██████████| 82/82 [00:50<00:00,  1.61it/s, bce_dice_loss - 1.498, iou_score - 0.5874]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 2\n",
            "train: 100%|██████████| 328/328 [06:19<00:00,  1.16s/it, bce_dice_loss - 1.493, iou_score - 0.634]\n",
            "valid: 100%|██████████| 82/82 [00:51<00:00,  1.61it/s, bce_dice_loss - 1.499, iou_score - 0.5788]\n",
            "\n",
            "Epoch: 3\n",
            "train: 100%|██████████| 328/328 [06:19<00:00,  1.16s/it, bce_dice_loss - 1.492, iou_score - 0.6427]\n",
            "valid: 100%|██████████| 82/82 [00:51<00:00,  1.59it/s, bce_dice_loss - 1.491, iou_score - 0.6532]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 4\n",
            "train: 100%|██████████| 328/328 [06:19<00:00,  1.16s/it, bce_dice_loss - 1.492, iou_score - 0.6446]\n",
            "valid: 100%|██████████| 82/82 [00:47<00:00,  1.73it/s, bce_dice_loss - 1.494, iou_score - 0.6245]\n",
            "\n",
            "Epoch: 5\n",
            "train: 100%|██████████| 328/328 [06:21<00:00,  1.16s/it, bce_dice_loss - 1.492, iou_score - 0.6412]\n",
            "valid: 100%|██████████| 82/82 [00:50<00:00,  1.61it/s, bce_dice_loss - 1.495, iou_score - 0.6159]\n",
            "Decrease decoder learning rate to 1e-3!\n",
            "\n",
            "Epoch: 6\n",
            "train:  11%|█         | 36/328 [00:43<05:51,  1.20s/it, bce_dice_loss - 1.493, iou_score - 0.6306]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-3a0df6c92024>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nEpoch: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mvalid_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_epoch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/segmentation_models_pytorch/utils/train.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, dataloader)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0;31m# update loss logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/segmentation_models_pytorch/utils/train.py\u001b[0m in \u001b[0;36mbatch_update\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhveOi36FAKU"
      },
      "source": [
        "# Create instances of loss function and optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jP0goxOfKm8R"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yvoWgsmFNX_"
      },
      "source": [
        "# Load the saved model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNL9T_fIFTgR"
      },
      "source": [
        "# import segmentation_models_pytorch as smp\n",
        "\n",
        "# # path of saved best model\n",
        "# model_path = './saved/best_model.pt'\n",
        "\n",
        "# # initialize the model\n",
        "# model = smp.Unet(encoder_name='efficientnet-b1', classes=12 , encoder_weights=None, activation=None).to(device)\n",
        "\n",
        "# # load the saved best model\n",
        "# checkpoint = torch.load(model_path, map_location=device)\n",
        "# state_dict = checkpoint.state_dict()\n",
        "# model.load_state_dict(state_dict)\n",
        "\n",
        "# # switch to evaluation mode\n",
        "# model.eval()\n",
        "# print('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIw8_NC9Mtee"
      },
      "source": [
        " # Inference\n",
        " Infer one sample and show the our model prediction result to make sure our model works correctly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiHX-B6d03-k"
      },
      "source": [
        "# outs=model(temp_images[0].cuda().unsqueeze(0))\n",
        "# oms = torch.argmax(outs, dim=1).detach().cpu().numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Es1bnd6S5Z7r"
      },
      "source": [
        "# np.stack(temp_masks).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bTcLHMH4wM7"
      },
      "source": [
        "def _threshold(x, threshold=None):\n",
        "    if threshold is not None:\n",
        "        return (x > threshold).type(x.dtype)\n",
        "    else:\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbviVc5RS3UJ"
      },
      "source": [
        "for imgs, image_infos in test_loader:\n",
        "    image_infos = image_infos\n",
        "    temp_images = imgs\n",
        "    \n",
        "    model.eval()\n",
        "    # inference\n",
        "    outs = model(torch.stack(temp_images).to(device))\n",
        "    outs = _threshold(outs,0.7)\n",
        "    oms = torch.argmax(outs.squeeze(), dim=1).detach().cpu().numpy()\n",
        "    \n",
        "   \n",
        "    for i in range(4):\n",
        "        fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(16, 16))\n",
        "\n",
        "        print('Shape of Original Image :', list(temp_images[i].shape))\n",
        "        print('Shape of Predicted : ', list(oms[i].shape))\n",
        "        print('Unique values, category of transformed mask : \\n', [{int(i),category_names[int(i)]} for i in list(np.unique(oms[i]))])\n",
        "\n",
        "        # Original image\n",
        "        ax1.imshow(temp_images[i].permute([1,2,0]))\n",
        "        ax1.grid(False)\n",
        "        ax1.set_title(\"Original image : {}\".format(image_infos[i]['file_name']), fontsize = 15)\n",
        "\n",
        "        # Predicted mask\n",
        "        ax2.imshow(oms[i])\n",
        "        ax2.grid(False)\n",
        "        ax2.set_title(\"Predicted : {}\".format(image_infos[i]['file_name']), fontsize = 15)\n",
        "\n",
        "        plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5N7u6ysAKJM"
      },
      "source": [
        "\n",
        "\n",
        "# val_dataset = CustomDataset(data_dir=val_path, mode='val', transform=get_validation_augmentation(),preprocessing=get_preprocessing(preprocessing_fn))\n",
        "\n",
        "# val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
        "#                                          batch_size=2,\n",
        "#                                          shuffle=True,\n",
        "#                                          num_workers=1,\n",
        "#                                         )\n",
        "# for imgs, masks in val_loader:\n",
        "#     mask = arr2mask(masks)\n",
        "#     temp_images = imgs\n",
        "#     model.eval()\n",
        "#     # inference\n",
        "#     outs = model(temp_images.to(device))\n",
        "#     outs = _threshold(outs,0.8)\n",
        "#     oms = torch.argmax(outs.squeeze(), dim=1).detach().cpu().numpy()\n",
        "    \n",
        "   \n",
        "#     for i in range(2):\n",
        "#         fig, (ax1, ax2,ax3) = plt.subplots(nrows=1, ncols=3, figsize=(16, 16))\n",
        "\n",
        "#         print('Shape of Original Image :', list(temp_images[i].shape))\n",
        "#         print('Shape of Predicted : ', list(oms[i].shape))\n",
        "#         print('Unique values, category of transformed mask : \\n', [{int(i),category_names[int(i)]} for i in list(np.unique(oms[i]))])\n",
        "\n",
        "#         # Original image\n",
        "#         ax1.imshow(temp_images[i].permute([1,2,0]))\n",
        "#         ax1.grid(False)\n",
        "#         #ax1.set_title(\"Original image : {}\".format(image_infos[i]['file_name']), fontsize = 15)\n",
        "\n",
        "#         # Predicted mask\n",
        "#         ax2.imshow(oms[i])\n",
        "#         ax2.grid(False)\n",
        "        \n",
        "#         ax3.imshow(mask[i])\n",
        "#         ax3.grid(False)\n",
        "#         #ax2.set_title(\"Predicted : {}\".format(image_infos[i]['file_name']), fontsize = 15)\n",
        "\n",
        "#         plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-yONDQcCZRr"
      },
      "source": [
        "mIoU_list = []\n",
        "for imgs, masks in val_loader:\n",
        "    outs = model(imgs.to(device))\n",
        "    outs = _threshold(outs,0.7)\n",
        "    oms = torch.argmax(outs.squeeze(), dim=1).detach().cpu().numpy()\n",
        "    masks = arr2mask(masks)\n",
        "    mIoU = label_accuracy_score(masks,oms)\n",
        "    mIoU_list.append(mIoU)\n",
        "    #print('mIoU: {:.4f}'.format(mIoU))\n",
        "\n",
        "\n",
        "print('mIoU: {:.4f}'.format(np.mean(mIoU_list)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ga0MBgD3S_iI"
      },
      "source": [
        "# Define the test function to submit model prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8UJe53xS8Q0"
      },
      "source": [
        "def test(model, data_loader, device):\n",
        "    size = 256\n",
        "    transform = A.Compose([A.Resize(256, 256)])\n",
        "    print('Start prediction.')\n",
        "    model.eval()\n",
        "    \n",
        "    file_name_list = []\n",
        "    preds_array = np.empty((0, size*size), dtype=np.long)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for step, (imgs, image_infos) in enumerate(test_loader):\n",
        "\n",
        "            # inference (512 x 512)\n",
        "            outs = model(torch.stack(imgs).to(device))\n",
        "            outs = _threshold(outs,0.7)\n",
        "            oms = torch.argmax(outs.squeeze(), dim=1).detach().cpu().numpy()\n",
        "            \n",
        "            # resize (256 x 256)\n",
        "            temp_mask = []\n",
        "            for img, mask in zip(np.stack(temp_images), oms):\n",
        "                transformed = transform(image=img, mask=mask)\n",
        "                mask = transformed['mask']\n",
        "                temp_mask.append(mask)\n",
        "\n",
        "            oms = np.array(temp_mask)\n",
        "            \n",
        "            oms = oms.reshape([oms.shape[0], size*size]).astype(int)\n",
        "            preds_array = np.vstack((preds_array, oms))\n",
        "            \n",
        "            file_name_list.append([i['file_name'] for i in image_infos])\n",
        "    print(\"End prediction.\")\n",
        "    file_names = [y for x in file_name_list for y in x]\n",
        "    \n",
        "    return file_names, preds_array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlQ8RNa5TC0Y"
      },
      "source": [
        "# Make a submission"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZUgLi7ATExx"
      },
      "source": [
        "# inference\n",
        "file_names, preds = test(model, test_loader, device)\n",
        "submission = pd.DataFrame()\n",
        "\n",
        "submission['image_id'] = file_names\n",
        "submission['PredictionString'] = [' '.join(str(e) for e in string.tolist()) for string in preds]\n",
        "\n",
        "# save submission.csv\n",
        "submission.to_csv(\"submission.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTy4AewxXpEr"
      },
      "source": [
        "# Submit to the leaderboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rOuiFjrXpEr"
      },
      "source": [
        "You have to submit your 'submission.csv' file to the leaderboard. You can connect directly to the submission page using the link below.\n",
        "http://dev.stages.ai/competitions/15/submission"
      ]
    }
  ]
}